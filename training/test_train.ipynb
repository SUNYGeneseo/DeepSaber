{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of the model\n",
    "## Extra lib instalation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install speechpy\n",
    "!pip install soundfile\n",
    "!pip install tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lib imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import scipy.io.wavfile as wav\n",
    "import speechpy\n",
    "import json\n",
    "import os\n",
    "import math\n",
    "import tensorflow.keras as k\n",
    "import dask.dataframe as dd\n",
    "from IPython.display import display, Markdown\n",
    "from time import sleep\n",
    "from pprint import pprint\n",
    "from multiprocessing import Queue, Process, Pool\n",
    "\n",
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For audio processing I will use `speechpy` as it is the fastest of the well known libraries:\n",
    "\n",
    "![speed_comp](https://camo.githubusercontent.com/1465ddaba9f99df4ff86ef800ef4598f35c12698/68747470733a2f2f696d61676573322e696d67626f782e636f6d2f64652f31302f6f4e5a776b49694b5f6f2e706e67)\n",
    "> Source: [sonopy](https://github.com/MycroftAI/sonopy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing function for dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def df_col2numpy(df, col_names):\n",
    "    ser = df.apply(lambda row: np.array([row[col] for col in col_names]).flatten(), axis=1)\n",
    "    arr = np.array(ser.values.tolist())\n",
    "    return arr\n",
    "\n",
    "def df_col2series(df, col_names):\n",
    "    if len(col_names) == 1:\n",
    "        ser = df[col_names[0]].map(lambda cell: np.array([cell]).flatten())\n",
    "    ser = df.apply(lambda row: np.array([row[col] for col in col_names]).flatten(), axis=1)\n",
    "    return ser\n",
    "\n",
    "def file2mfcc(file_name, frame_length=0.20, frame_stride=0.1, recreate=False):\n",
    "    \"\"\" recreate: whether to recreate existing .npy MFCC\"\"\"\n",
    "    \n",
    "    dir_name = file_name[:file_name.index('blocks')]\n",
    "    file_wav, file_ogg = None, None\n",
    "    \n",
    "#     check for existing .wav or .npy cache\n",
    "    for file in os.listdir(dir_name):\n",
    "#         if file.endswith('.wav'):\n",
    "#             file_wav = os.path.join(dir_name, file)\n",
    "#         if file.endswith('.npy') and not recreate:\n",
    "#             return np.load(os.path.join(dir_name, file))\n",
    "        if file.endswith('.mfcc') and not recreate:\n",
    "            return pd.read_hdf((os.path.join(dir_name, file)))\n",
    "            \n",
    "#     if none .wav found, create it\n",
    "    for file in os.listdir(dir_name):\n",
    "        if file.endswith('.ogg'):\n",
    "            file_ogg = os.path.join(dir_name, file)\n",
    "            if not file_wav:\n",
    "                data, samplerate = sf.read(file_ogg)\n",
    "                file_wav = f'{file_ogg[:-4]}.wav'\n",
    "                sf.write(file_wav, data, samplerate)\n",
    "\n",
    "    fs, signal = wav.read(file_wav)\n",
    "    \n",
    "#     Stereo to mono\n",
    "    if signal.shape[1] == 2:\n",
    "        signal = (signal[:, 0] + signal[:, 1]) / 2\n",
    "    else:\n",
    "        signal = signal[:, 0]\n",
    "\n",
    "    # Pre-emphasize\n",
    "    signal_preemphasized = speechpy.processing.preemphasis(signal, cof=0.98)\n",
    "\n",
    "    # Extract MFCC features\n",
    "    mfcc = speechpy.feature.mfcc(signal, sampling_frequency=fs, frame_length=frame_length, \n",
    "                                 frame_stride=frame_stride, num_filters=40, fft_length=512,\n",
    "                                 low_frequency=0, high_frequency=None, num_cepstral=13)\n",
    "    \n",
    "#     Normalize\n",
    "    mfcc_cmvn = speechpy.processing.cmvnw(mfcc,win_size=301,variance_normalization=True)\n",
    "    \n",
    "#     Cache results and clean .wav to save space\n",
    "#     np.save(f'{file_wav[:-4]}.mfcc.npy', mfcc_cmvn)\n",
    "    if file_ogg:\n",
    "        os.remove(file_wav)\n",
    "\n",
    "#     Recalculate the time differences\n",
    "    index = np.arange(0, (len(mfcc) - 0.5) * frame_stride, frame_stride) + frame_length\n",
    "    df = pd.DataFrame(data=mfcc_cmvn, index=index).apply(np.array, axis=1)\n",
    "    df.to_hdf(f'{file_wav[:-4]}.mfcc', 'mfcc', mode='w', format='fixed')\n",
    "    return df\n",
    "\n",
    "def process_cell(cell, side):\n",
    "    res = cell[:, :, side, :]\n",
    "    \n",
    "    mx = res.max()\n",
    "    mx_index = np.unravel_index(res.argmax(), res.shape)\n",
    "    pred = [None for _ in range(3)]\n",
    "    \n",
    "    for dim in range(3):\n",
    "#         pred[dim] = np.zeros(res.shape[dim] + 1)\n",
    "        pred[dim] = np.zeros(res.shape[dim])\n",
    "        \n",
    "        if mx < 0.5:\n",
    "#             pred[dim][-1] = 1\n",
    "            pass\n",
    "        else:\n",
    "            pred[dim][mx_index[dim]] = 1\n",
    "    \n",
    "    if mx < 0.5:\n",
    "        res = cell[:, :, (side+1) % 2, :]\n",
    "    \n",
    "        mx = res.max()\n",
    "        mx_index = np.unravel_index(res.argmax(), res.shape)\n",
    "        for dim in range(3):\n",
    "            pred[dim][mx_index[dim]] = 1\n",
    "        \n",
    "    return pred\n",
    "\n",
    "def change_output(df: pd.DataFrame):\n",
    "    left, right = df.apply(lambda cell: process_cell(cell, 0)), df.apply(lambda cell: process_cell(cell, 1))\n",
    "    \n",
    "    left = pd.DataFrame(left.to_list(), columns=[f'l_dim{x}' for x in range(3)], index=left.index)\n",
    "    right = pd.DataFrame(right.to_list(), columns=[f'r_dim{x}' for x in range(3)], index=right.index)\n",
    "    \n",
    "    return left.join(right)\n",
    "    \n",
    "def process_file(file_path, recreate=False):\n",
    "    print(f'Processing {file_path}')\n",
    "    try:\n",
    "        df = pd.read_pickle(file_path)\n",
    "    \n",
    "        # all in one serialization (99.5 % with bad metric)\n",
    "        #     df['output'] = df_col2series(df, ['output'])\n",
    "        df = df.join(change_output(df['output']))\n",
    "        df['shifted'] = df['output'].shift(1, fill_value=[np.zeros(df['output'].iloc[0].shape)])\n",
    "        df['times'] = df_col2series(df, ['prev', 'next'])\n",
    "        df['name'] = f'{file_path}'\n",
    "\n",
    "        mfcc = file2mfcc(file_path, recreate=recreate)\n",
    "        mfcc.name = 'mfcc'\n",
    "        round_index = mfcc.index.values[1] - mfcc.index.values[0]\n",
    "        df.index = np.floor(df['time'] / round_index).astype(int)\n",
    "        mfcc.index = (mfcc.index / round_index).astype(int)\n",
    "\n",
    "        df = df.join(mfcc)\n",
    "        df.index = df['time']\n",
    "        df = df.dropna()\n",
    "    except Exception as e:\n",
    "        print(f'Caught Error: {e}')\n",
    "        return None\n",
    "\n",
    "    return df\n",
    "    \n",
    "file = '../data/Army Of The Night/blocks/Expert.pkl'\n",
    "file2mfcc(file, recreate=False)\n",
    "process_file(file).iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blocks generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_paths(path, hard_max):\n",
    "    \"\"\" \n",
    "    Create a list of all pregenerated blocks files \n",
    "    Search in full subtree of :path:\n",
    "    \"\"\"\n",
    "    \n",
    "    file_paths = []\n",
    "    counter = 0\n",
    "    \n",
    "    for root, dirs, files in os.walk(path, topdown=False):\n",
    "        if counter > hard_max:\n",
    "                break\n",
    "        for name in files:\n",
    "            if root[-6:] == 'blocks':\n",
    "                print(f'#{counter:5} {root}/{name}')\n",
    "                file_paths.append(os.path.join(root, name))\n",
    "                \n",
    "                counter += 1\n",
    "                \n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set which cols to use as X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_list = []\n",
    "Y = []\n",
    "HARD_MAX = 20000\n",
    "path = '../data'\n",
    "\n",
    "X_cols = ['times', 'mfcc']\n",
    "y_cols = [f'{side}_dim{i}' for side in 'rl' for i in range(3) ]\n",
    "columns = ['name', 'time'] + X_cols + y_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and save training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pool = Pool(processes=None)\n",
    "# X_list = [process_file(x) for x in get_file_paths(path, HARD_MAX)]\n",
    "X_list = pool.map(process_file, get_file_paths(path, HARD_MAX))\n",
    "X_list = [x for x in X_list if x is not None]\n",
    "        \n",
    "print(f'Passes {len(X_list):6}/{HARD_MAX:6} hard max')\n",
    "X = pd.DataFrame(pd.concat(X_list), columns=columns)\n",
    "X = X.set_index(['name', 'time'])\n",
    "\n",
    "X.to_pickle(os.path.join(path, 'X_saved.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_pickle(os.path.join(path, 'X_saved.gzip'))\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for train_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "precision = 2\n",
    "\n",
    "def pp(row):\n",
    "    print('*' * 69)\n",
    "    print(row)\n",
    "    \n",
    "def round_up(num:float, prec: int) -> int:\n",
    "    return int(math.ceil((10 ** -prec) * num) / (10 ** -prec))\n",
    "    \n",
    "def get_len_category(song, prec: int=precision):\n",
    "    return int(round_up(len(song), 2))\n",
    "\n",
    "def get_mask(X, prec: int=precision):\n",
    "    mask = X.groupby('name').apply(get_len_category)\n",
    "    return mask.to_dict()\n",
    "\n",
    "def create_batch(group, ceil_len):\n",
    "    print(f'Creating batch of ceil_len {ceil_len:6} with {len(group)} rows')\n",
    "    ceil_len = int(ceil_len)\n",
    "    \n",
    "    batch = []\n",
    "    for name, song in group.groupby('name'):\n",
    "        empty_row = song.head(1).squeeze().apply(np.zeros_like)\n",
    "#         print(f'{ceil_len} | {len(song)} | {empty_row}')\n",
    "        df_to_add = pd.DataFrame([empty_row] * (ceil_len - len(song)))\n",
    "        batch.append(pd.concat([song, df_to_add]))\n",
    "\n",
    "    return pd.concat(batch)\n",
    "\n",
    "# LESON: Don't forget about the NaNs!\n",
    "\n",
    "\n",
    "def list2numpy(batch, col_name):\n",
    "    return np.array(batch.groupby('name')[col_name].apply(list).to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show bucketing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = X.groupby(get_mask(X), level=0)\n",
    "\n",
    "adjust = 6\n",
    "stats = []\n",
    "print(f'{\"from\":>{adjust}} ‒ {\"to\":>{adjust}}: {\"# of songs\":>{adjust*2}}')\n",
    "      \n",
    "for name, group in grouped:\n",
    "    print(f'{name - 10 ** precision:{adjust}} ‒ {name:{adjust}}: {len(group.groupby(\"name\").groups):{adjust*2}}')\n",
    "    stats.append({'from': name - 10 ** precision, 'to': name, '# of songs': len(group.groupby(\"name\").groups)})\n",
    "          \n",
    "# print in your favorite way\n",
    "# pd.DataFrame(stats, columns=['from', 'to', '# of songs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(df):\n",
    "    grouped = X.groupby(get_mask(df), level=0)\n",
    "    \n",
    "#     p = Pool(2)  # slowe because of memory\n",
    "#     batches = p.starmap(create_batch, [(group, ceil_len) for ceil_len, group in grouped])\n",
    "#     grouped = list(grouped)[:2]\n",
    "    batches = [create_batch(group, ceil_len) for ceil_len, group in grouped]\n",
    "    batches = [batch for batch in batches if len(batch.groupby('name').groups) > 8]\n",
    "    \n",
    "    while True:\n",
    "        for batch in batches:\n",
    "            yield [list2numpy(batch, col) for col in X_cols],\\\n",
    "                  [list2numpy(batch, col) for col in y_cols]\n",
    "\n",
    "# test generated shapes\n",
    "generator = train_generator(X)\n",
    "for x, y in generator:\n",
    "    print(f'x.shapes {[np.array(x_t).shape for x_t in x]}')\n",
    "    print(f'y.shape {[np.array(y_t).shape for y_t in y]}\\n')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, LSTM, Flatten, Input, Activation, TimeDistributed, concatenate\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(X):\n",
    "    demo_row = X.iloc[0]\n",
    "    X_shapes = [demo_row[col].shape[0] for col in X_cols]\n",
    "    y_shapes = [demo_row[col].shape[0] for col in y_cols]\n",
    "    \n",
    "#     in1 = Input(shape=(None, 216)) # last blocks\n",
    "#     in2 = Input(shape=(None, 2))   # time difference of previous and next beat\n",
    "    inputs = [Input(shape=(None, shape)) for shape in X_shapes]\n",
    "    \n",
    "    time_dist = [TimeDistributed(Dense(shape, activation='elu'))(inputs[i]) for i, shape in enumerate(X_shapes)]\n",
    "#     x1 = TimeDistributed(Dense(50, activation='elu'))(in1)\n",
    "#     x2 = TimeDistributed(Dense(3, activation='elu'))(in2)\n",
    "    \n",
    "    out = concatenate(time_dist, axis=-1)\n",
    "    out = LSTM(128, return_sequences=True)(out)\n",
    "    out = LSTM(64, return_sequences=True)(out)\n",
    "    out = LSTM(128, return_sequences=True)(out)\n",
    "#     out = TimeDistributed(Dense(216, activation='sigmoid'))(out)\n",
    "    outputs = [TimeDistributed(Dense(shape, activation='softmax'), name=col)(out) for shape, col in zip(y_shapes, y_cols)]\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(optimizer='rmsprop',\n",
    "#                   loss='binary_crossentropy',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  loss_weights=[1,2,3,1,2,3],\n",
    "                  metrics=['accuracy', 'categorical_crossentropy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a new model and show it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(X)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model on generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "hard_counter = 0\n",
    "hard_max = 120\n",
    "\n",
    "def important_metric(metric_name):\n",
    "    return 'val' in metric_name and 'acc' in metric_name\n",
    "        \n",
    "\n",
    "acc_results = {}\n",
    "stats_len = 30\n",
    "\n",
    "generator = train_generator(X)\n",
    "for i, (x, y) in enumerate(generator):\n",
    "#     print(f'x.shapes {[np.array(x_t).shape for x_t in x]}  |  \\ny.shape {[np.array(y_t).shape for y_t in y]}')\n",
    "    if counter > 14 or hard_counter > hard_max:\n",
    "        break\n",
    "    res = model.fit(x, y, batch_size=128, validation_split=0.1, verbose=False)\n",
    "\n",
    "    if i % stats_len == 0:\n",
    "        \n",
    "        total_acc = (np.array(list(acc_results.values()))/stats_len).mean()\n",
    "        display(Markdown(f'### Batch {i:4} | {total_acc:4.4}'))\n",
    "        \n",
    "        \n",
    "        pprint([f'{key:30}: {val/stats_len}' for key, val in acc_results.items()])\n",
    "        \n",
    "        acc_results = {key: val[0] for key, val in res.history.items() if important_metric(key)}\n",
    "    else:\n",
    "        for key in acc_results:\n",
    "            acc_results[key] += res.history[key][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvements\n",
    "\n",
    "1. More normalization of the data\n",
    "    - Force the model to catch the underlying principle and not \"mean\"\n",
    "1. Flip L / R hand and horizontal mirorring and rotations\n",
    "1. Flip vertically with rotation\n",
    "1. If one hand not used, mirror the other hand instead of \"0\"\n",
    "1. Instead of generator, create snippets of 100 beats\n",
    "    - Easier GPU training\n",
    "    - Train it on gColab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hand evaluation\n",
    "- Is needed since good results can be caused by a wrongly chosen matric!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "generator = train_generator(X)\n",
    "x, y = generator.__next__()\n",
    "prediction = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, t = 0, 20\n",
    "for dim, (p, y_t) in enumerate(zip(prediction, y)):\n",
    "    \n",
    "    df = pd.DataFrame(p[0][f:t])\n",
    "#     df = df.diff(-1)\n",
    "    df = df.eq(df.where(df != 0).max(1), axis=0).astype(int)\n",
    "    df.index.name = y_cols[dim]\n",
    "    df_y = pd.DataFrame(y_t[0][f:t]).astype(int)\n",
    "    df = df.join(df_y, rsuffix='_true')\n",
    "    display(df)\n",
    "#     display(df_y)\n",
    "# print(y.shape)\n",
    "# model2 = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_generator():\n",
    "#     while True:\n",
    "#         for x1, x2, y in zip(X['blocks'][:300], X['times'][:300], Y[:300]):\n",
    "#             yield [np.array([x1, ]), np.array([x2, ])], np.array([y, ])\n",
    "            \n",
    "def eval_generator():\n",
    "    while True:\n",
    "        for x1, x2, y in zip(X['blocks'][300:], X['times'][300:], Y[300:]):\n",
    "            yield [np.array([x1, ]), np.array([x2, ])], np.array([y, ])\n",
    "            \n",
    "model = get_model()\n",
    "model.batch_size = 8            \n",
    "\n",
    "model.fit_generator(train_generator(), steps_per_epoch=300, epochs=1, verbose=1, \n",
    "                    use_multiprocessing=False)\n",
    "model.evaluate_generator(eval_generator(), steps=19, )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
